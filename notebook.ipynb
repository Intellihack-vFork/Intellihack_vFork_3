{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##1. Data Generation & Dataset Creation"
      ],
      "metadata": {
        "id": "83_3GPGhxY57"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###1.1. Data Loading and Preprocessing\n",
        "\n",
        "make sure to upload md and pdf files into `/data` directory"
      ],
      "metadata": {
        "id": "vfraKSxByBL9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "from PyPDF2 import PdfReader\n",
        "\n",
        "def load_and_preprocess_data(data_dir):\n",
        "    \"\"\"Loads and preprocesses Markdown and PDF files.\"\"\"\n",
        "    documents = []\n",
        "    for filename in os.listdir(data_dir):\n",
        "        filepath = os.path.join(data_dir, filename)\n",
        "        if filename.endswith(\".md\"):\n",
        "            with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
        "                text = f.read()\n",
        "                text = re.sub(r\"^[#\\s]+\", \"\", text, flags=re.MULTILINE)\n",
        "                documents.append({\"filename\": filename, \"text\": text})\n",
        "        elif filename.endswith(\".pdf\"):\n",
        "            try:\n",
        "                with open(filepath, \"rb\") as f:\n",
        "                    pdf_reader = PdfReader(f)\n",
        "                    text = \"\"\n",
        "                    for page in pdf_reader.pages:\n",
        "                        text += page.extract_text()\n",
        "                    # cleaning\n",
        "                    text = re.sub(r\"\\n+\", \"\\n\", text)\n",
        "                    text = re.sub(r\" +\", \" \", text)\n",
        "                    documents.append({\"filename\": filename, \"text\": text})\n",
        "            except Exception as e:\n",
        "                print(f\"Error reading PDF {filename}: {e}\")\n",
        "    return documents\n",
        "\n",
        "\n",
        "data_dir = \"data\"\n",
        "documents = load_and_preprocess_data(data_dir)\n",
        "# print(documents[0]['text'][:500]) # check a document"
      ],
      "metadata": {
        "id": "_FjXHjHKyDfQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###1.2. Synthetic Data Generation (using Qwen2.5-3B-Instruct itself, initially)"
      ],
      "metadata": {
        "id": "CqxY2vVV0lk3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline, AutoTokenizer\n",
        "\n",
        "def generate_qa_pairs(documents, model_name=\"Qwen/Qwen2.5-3B-Instruct\", num_questions_per_doc=5):\n",
        "    \"\"\"Generates QA pairs using the base Qwen model.\"\"\"\n",
        "\n",
        "    generator = pipeline('text-generation', model=model_name, device=0)  # Use GPU if available\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    qa_pairs = []\n",
        "    for doc in documents:\n",
        "\n",
        "        text_chunks = [doc['text'][i:i+4096] for i in range(0, len(doc['text']), 4096)]\n",
        "\n",
        "        for chunk in text_chunks:\n",
        "            prompt = f\"\"\"\n",
        "            Context:\n",
        "            {chunk}\n",
        "\n",
        "            Based on the above context, generate {num_questions_per_doc} question and answer pairs.\n",
        "            Format them strictly as follows:\n",
        "\n",
        "            Q: [Question 1]\n",
        "            A: [Answer 1]\n",
        "\n",
        "            Q: [Question 2]\n",
        "            A: [Answer 2]\n",
        "\n",
        "            ...\n",
        "            \"\"\"\n",
        "\n",
        "            # Generate text using model\n",
        "            generated_text = generator(\n",
        "                prompt,\n",
        "                max_length=1024,\n",
        "                num_return_sequences=1,\n",
        "                pad_token_id=tokenizer.eos_token_id,\n",
        "                do_sample=True,\n",
        "                top_k=50,\n",
        "                top_p=0.95,\n",
        "                temperature=0.7\n",
        "            )[0]['generated_text']\n",
        "\n",
        "\n",
        "            matches = re.findall(r\"Q: (.*?)\\nA: (.*?)(?=\\nQ:|\\Z)\", generated_text, re.DOTALL)\n",
        "            for question, answer in matches:\n",
        "                qa_pairs.append({\"question\": question.strip(), \"answer\": answer.strip(), \"source\": doc['filename']})\n",
        "    return qa_pairs\n",
        "\n",
        "\n",
        "initial_qa_pairs = generate_qa_pairs(documents, num_questions_per_doc=3)\n",
        "# print(initial_qa_pairs[:5])"
      ],
      "metadata": {
        "id": "jh0ELrAO0pQ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###1.3. Data Augmentation and Refinement"
      ],
      "metadata": {
        "id": "OBpkmB1J1ITt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "def paraphrase_qa(qa_pairs, model_name=\"google/flan-t5-base\"):\n",
        "  \"\"\"Paraphrases questions and answers for data augmentation.\"\"\"\n",
        "  paraphraser = pipeline(\"text2text-generation\", model=model_name, device=0) # Use GPU\n",
        "  augmented_qa_pairs = []\n",
        "\n",
        "  for pair in qa_pairs:\n",
        "\n",
        "    prompt_q = f\"paraphrase: {pair['question']}\"\n",
        "    paraphrased_question = paraphraser(prompt_q, max_length=128, do_sample=True, top_k=50, top_p=0.95, temperature=0.7)[0]['generated_text']\n",
        "\n",
        "\n",
        "    prompt_a = f\"paraphrase: {pair['answer']}\"\n",
        "    paraphrased_answer = paraphraser(prompt_a, max_length=256, do_sample=True, top_k=50, top_p=0.95, temperature=0.7)[0]['generated_text']\n",
        "\n",
        "    augmented_qa_pairs.append({\n",
        "      \"question\": paraphrased_question.strip(),\n",
        "      \"answer\": paraphrased_answer.strip(),\n",
        "      \"original_question\": pair['question'],\n",
        "      \"original_answer\": pair['answer'],\n",
        "      \"source\": pair['source']\n",
        "    })\n",
        "  return augmented_qa_pairs\n",
        "\n",
        "augmented_pairs = paraphrase_qa(initial_qa_pairs)\n",
        "all_qa_pairs = initial_qa_pairs + augmented_pairs"
      ],
      "metadata": {
        "id": "S8AhWD-k1VL1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###1.4. Dataset Splitting"
      ],
      "metadata": {
        "id": "Il4khaCg1jTP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "def split_dataset(qa_pairs, train_ratio=0.8, val_ratio=0.1):\n",
        "    \"\"\"Splits the dataset into training, validation, and test sets.\"\"\"\n",
        "    random.shuffle(qa_pairs)\n",
        "    train_size = int(len(qa_pairs) * train_ratio)\n",
        "    val_size = int(len(qa_pairs) * val_ratio)\n",
        "    train_data = qa_pairs[:train_size]\n",
        "    val_data = qa_pairs[train_size:train_size + val_size]\n",
        "    test_data = qa_pairs[train_size + val_size:]\n",
        "    return train_data, val_data, test_data\n",
        "\n",
        "train_data, val_data, test_data = split_dataset(all_qa_pairs)\n",
        "print(f\"Train size: {len(train_data)}, Val size: {len(val_data)}, Test size: {len(test_data)}\")"
      ],
      "metadata": {
        "id": "jHX6VMBI1mCE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###1.5 Dataset Formatting (JSONL)"
      ],
      "metadata": {
        "id": "yB7ZnTiy2tze"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "def create_jsonl_dataset(qa_pairs, filename):\n",
        "    \"\"\"Creates a JSONL dataset file.\"\"\"\n",
        "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
        "        for pair in qa_pairs:\n",
        "           prompt = f\"Question: {pair['question']}\\n\"\n",
        "           if 'context' in pair:\n",
        "             prompt += f\"Context: {pair['context']}\\n\"\n",
        "\n",
        "           # Create the JSON object\n",
        "           data_point = {\n",
        "               \"prompt\": prompt,\n",
        "               \"response\": pair['answer']\n",
        "           }\n",
        "           f.write(json.dumps(data_point) + \"\\n\")\n",
        "\n",
        "create_jsonl_dataset(train_data, \"train.jsonl\")\n",
        "create_jsonl_dataset(val_data, \"val.jsonl\")\n",
        "create_jsonl_dataset(test_data, \"test.jsonl\")"
      ],
      "metadata": {
        "id": "wKHz3ePj2yK9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2. Model Selection and Preparation"
      ],
      "metadata": {
        "id": "mNxrpTNK3MWY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "import torch\n",
        "\n",
        "model_name = \"Qwen/Qwen2.5-3B-Instruct\"\n",
        "\n",
        "# Quantization Configuration\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "# Load model and tokenizer\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",  # Use \"cuda\" if you have a GPU, \"cpu\" otherwise\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"left\""
      ],
      "metadata": {
        "id": "SeFOthn53QWH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3. Efficient Fine-tuning (QLoRA)"
      ],
      "metadata": {
        "id": "fhvWmsQE3iEn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "pip install --upgrade transformers"
      ],
      "metadata": {
        "id": "7_h6G-EU3kZu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel\n",
        "from trl import SFTTrainer\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = model_name,\n",
        "    max_seq_length = 2048,\n",
        "    dtype = torch.bfloat16,\n",
        "    load_in_4bit = True,\n",
        ")\n",
        "\n",
        "\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16,\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    lora_alpha = 32,\n",
        "    lora_dropout = 0.05,\n",
        "    bias = \"none\",\n",
        ")\n",
        "\n",
        "\n",
        "from datasets import load_dataset\n",
        "train_dataset = load_dataset(\"json\", data_files=\"train.jsonl\", split=\"train\")\n",
        "val_dataset = load_dataset(\"json\", data_files=\"val.jsonl\", split=\"train\")\n",
        "\n",
        "# Training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./qwen-finetuned\",  # Output directory\n",
        "    per_device_train_batch_size=4,  # Batch size per GPU\n",
        "    gradient_accumulation_steps=4,   # Accumulate gradients over several steps\n",
        "    learning_rate=2e-4,             # Learning rate\n",
        "    fp16=False,                     # Use bfloat16 (more stable, better for Qwen2)\n",
        "    bf16=True,\n",
        "    logging_steps=10,             # Log training information\n",
        "    save_steps=100,\n",
        "    evaluation_strategy=\"steps\",      # Evaluate during training\n",
        "    eval_steps=100,                # Evaluation interval\n",
        "    num_train_epochs=3,          # Number of training epochs (adjust as needed)\n",
        "    warmup_steps=100,              # Warmup steps for learning rate scheduler\n",
        "    lr_scheduler_type=\"cosine\",      # Learning rate scheduler\n",
        "    remove_unused_columns=False,    # Important!\n",
        "    report_to=\"tensorboard\",\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    dataset_text_field=\"prompt\",\n",
        "    packing=False,\n",
        "    max_seq_length=2048,\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "\n",
        "\n",
        "# Train the model\n",
        "trainer.train()\n",
        "\n",
        "# Save the LoRA adapters\n",
        "trainer.save_model(\"./qwen-finetuned-adapters\")"
      ],
      "metadata": {
        "id": "EDnwmbOK3nJs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}